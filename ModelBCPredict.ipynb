{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "looking-mercy",
   "metadata": {},
   "source": [
    "## Using the ONNX model file for predictions\n",
    "### In this Notebook we take the model trained, saved in ONNX format and we do some predictions using a sampled test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "false-cancer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import random as rn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# conda env: mlcpuv1\n",
    "import keras2onnx\n",
    "import onnxruntime as rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "modular-swimming",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.3.1\n",
      "ONNX runtime version 1.4.0\n",
      "keras2onnx version 1.7.0\n"
     ]
    }
   ],
   "source": [
    "# check TF version (> 2.3)\n",
    "print('TF version', tf.__version__)\n",
    "print('ONNX runtime version', rt.__version__)\n",
    "print('keras2onnx version', keras2onnx.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-export",
   "metadata": {},
   "source": [
    "### prepare the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rapid-carnival",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numbers of samples for (total, train, valid, test): 569 398 85 86\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset for test\n",
    "\n",
    "# we take the dataset from Sklearn\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "\n",
    "# I prefer working with Dataframe\n",
    "orig_df = data.frame\n",
    "\n",
    "# we must rename columns, to remove spaces in names\n",
    "# otherwise we get problems with ONNX\n",
    "\n",
    "# substitute all spaces with _\n",
    "dict_columns = {}\n",
    "\n",
    "for col in orig_df.columns:\n",
    "    dict_columns[col] = col.replace(\" \", \"_\")\n",
    "\n",
    "orig_df = orig_df.rename(columns=dict_columns)\n",
    "\n",
    "# Split the dataset in train, valid, test\n",
    "\n",
    "N_TOTAL = orig_df.shape[0]\n",
    "FRAC_TRAIN = 0.7\n",
    "FRAC_VALID = 0.15\n",
    "\n",
    "N_TRAIN = int(N_TOTAL * FRAC_TRAIN)\n",
    "N_VALID = int(N_TOTAL * FRAC_VALID)\n",
    "N_TEST = N_TOTAL - N_TRAIN - N_VALID\n",
    "\n",
    "print('Numbers of samples for (total, train, valid, test):', N_TOTAL, N_TRAIN, N_VALID, N_TEST)\n",
    "\n",
    "# shuffle the data\n",
    "orig_df = orig_df.sample(frac=1.)\n",
    "\n",
    "df_train = orig_df.iloc[:N_TRAIN]\n",
    "df_valid = orig_df.iloc[N_TRAIN:N_TRAIN+N_VALID]\n",
    "df_test = orig_df.iloc[N_TRAIN+N_VALID:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "extreme-profession",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to convert in TF dataset\n",
    "#adapted from https://www.tensorflow.org/tutorials/structured_data/feature_columns\n",
    "def df_to_dataset(df, predictor,  batch_size=32, shuffle=True):\n",
    "    df = df.copy()\n",
    "    labels = df.pop(predictor)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    \n",
    "    if shuffle:\n",
    "        # don't shuffle test\n",
    "        ds = ds.shuffle(buffer_size=len(df))\n",
    "        \n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "portable-storage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we take only the test dataset\n",
    "ds_test = df_to_dataset(df_test, 'target', batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "located-nursery",
   "metadata": {},
   "source": [
    "### load the ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "moderate-cornell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OK\n"
     ]
    }
   ],
   "source": [
    "# load the ONNX model\n",
    "ONNX_MODEL_FILE = 'modelbc-artifact/modelbc.onnx'\n",
    "\n",
    "# first, a function to load the model and create an ONNX session\n",
    "def create_session(onnx_file_name, print_info=False):\n",
    "    sess = rt.InferenceSession(onnx_file_name)\n",
    "    \n",
    "    print('Loading OK')\n",
    "    \n",
    "    if print_info:\n",
    "        print(\"ONNX model expects \", len(sess.get_inputs()), 'features:')\n",
    "        # prints names of features\n",
    "        for n, input in enumerate(sess.get_inputs()):\n",
    "            print(input.name)\n",
    "        \n",
    "    return sess\n",
    "\n",
    "# build the input as expected from ONNX runtime\n",
    "def build_input_feed(f_batch):\n",
    "    # input: the features_batch as extracted from TF dataset\n",
    "    # devo costruire il dict come se lo aspetta onnx\n",
    "    \n",
    "    input_dict = {}\n",
    "    \n",
    "    # ogni feature Ã¨ un singolo valore\n",
    "    for col in f_batch.keys():\n",
    "        # get the numpy array of values\n",
    "        values = f_batch[col].numpy()\n",
    "        n_rows = len(values)\n",
    "        input_dict[col] = values.reshape((n_rows, 1))\n",
    "    \n",
    "    # input_feed is the dictionary input to ONNX model\n",
    "    # for every feature a column vector (n_rows, 1)\n",
    "    return input_dict\n",
    "\n",
    "def onnx_predict(f_batch, sess=create_session(ONNX_MODEL_FILE)):\n",
    "    # transform the input\n",
    "    input_feed = build_input_feed(f_batch)\n",
    "    \n",
    "    # run inference\n",
    "    pred_onnx = sess.run(None, input_feed)\n",
    "    \n",
    "    # build output\n",
    "    output = {}\n",
    "    output['probs'] = pred_onnx[0]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-scott",
   "metadata": {},
   "source": [
    "### do the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "finnish-mother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch n. 1\n",
      "Predictions: [9.9996328e-01 2.1381915e-02 9.9985421e-01 3.5762787e-06 9.3315327e-01\n",
      " 9.9879181e-01 1.0000000e+00 9.9731231e-01 1.7906627e-01 3.2621622e-04\n",
      " 9.9987954e-01 9.9990857e-01 1.0621250e-03 2.5789142e-03 9.9996686e-01\n",
      " 9.9999344e-01]\n",
      "Time (sec.) for batch prediction: 0.001\n",
      "\n",
      "Batch n. 2\n",
      "Predictions: [9.9691486e-01 9.9984485e-01 9.9589956e-01 1.9285083e-04 7.7962875e-05\n",
      " 9.9998009e-01 0.0000000e+00 9.9987197e-01 0.0000000e+00 0.0000000e+00\n",
      " 9.9915004e-01 9.9999642e-01 9.9996775e-01 2.3880601e-04 9.9728799e-01\n",
      " 9.9998200e-01]\n",
      "Time (sec.) for batch prediction: 0.003\n",
      "\n",
      "Batch n. 3\n",
      "Predictions: [2.9720610e-01 0.0000000e+00 9.9977744e-01 9.9484450e-01 6.7836046e-04\n",
      " 9.9987328e-01 4.1628748e-02 9.9999833e-01 2.4653971e-03 9.7196364e-01\n",
      " 9.9999535e-01 9.9933505e-01 9.9995828e-01 9.9156666e-01 9.9999392e-01\n",
      " 9.9244332e-01]\n",
      "Time (sec.) for batch prediction: 0.001\n",
      "\n",
      "Batch n. 4\n",
      "Predictions: [3.0377507e-04 0.0000000e+00 1.0000000e+00 9.9999702e-01 4.9226868e-01\n",
      " 1.0584801e-02 9.9976373e-01 2.0006895e-03 9.9971187e-01 8.3114630e-01\n",
      " 7.3611736e-06 9.9972421e-01 1.5795231e-05 9.9975312e-01 7.2748333e-01\n",
      " 6.2584877e-07]\n",
      "Time (sec.) for batch prediction: 0.001\n",
      "\n",
      "Batch n. 5\n",
      "Predictions: [9.9805176e-01 9.9999923e-01 0.0000000e+00 9.9954879e-01 3.2871962e-05\n",
      " 7.9095364e-05 9.9108285e-01 9.5221651e-01 1.0132790e-06 3.2036006e-03\n",
      " 9.9995595e-01 9.4077361e-01 1.2028217e-04 1.9523826e-01 9.9439842e-01\n",
      " 9.9757218e-01]\n",
      "Time (sec.) for batch prediction: 0.001\n",
      "\n",
      "Batch n. 6\n",
      "Predictions: [0.99956256 0.96708    0.99995273 0.00280777 0.99945784 0.0336487 ]\n",
      "Time (sec.) for batch prediction: 0.002\n",
      "\n",
      "\n",
      "ONNX test OK.\n"
     ]
    }
   ],
   "source": [
    "# this way I take only the feature batch out of a dataset batch\n",
    "\n",
    "# do the test on the entire test dataset\n",
    "for i, f_batch in enumerate(iter(ds_test)):\n",
    "    print('Batch n.', i+1)\n",
    "    \n",
    "    # needed\n",
    "    f_batch = f_batch[0]\n",
    "    \n",
    "    tStart = time.time()\n",
    "    \n",
    "    onnx_probs = onnx_predict(f_batch)\n",
    "    tEla = time.time() - tStart\n",
    "    print('Predictions:', onnx_probs['probs'].ravel())\n",
    "    \n",
    "    print('Time (sec.) for batch prediction:', round(tEla, 3))\n",
    "    print('')\n",
    "    \n",
    "print('')\n",
    "print('ONNX test OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-shelter",
   "metadata": {},
   "source": [
    "#### Ok, to do a prediction on 16 samples it takes around 2 msec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "remarkable-student",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
